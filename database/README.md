# Database Schema - Bulk-Optimized Event Storage

**PostgreSQL schema designed for high-volume bulk insert processing with client attribution**

## ðŸ“ Schema Files

```
database/
â”œâ”€â”€ 01_init.sql          # Core event storage optimized for bulk processing
â””â”€â”€ README.md           # This documentation
```

## ðŸ—ï¸ Architecture Overview

### Performance-First Design
```
Raw Events (tracking.js) 
       â†“ Client Attribution (pixel-management)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   events_log    â”‚ â† Bulk insert optimization (50-100x faster)
â”‚   (01_init.sql) â”‚ â† JSONB storage + client_id attribution
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“ Automated S3 Export (hourly)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client S3 Bucketâ”‚ â† Complete data ownership
â”‚ + Backup Bucket â”‚ â† Metering and compliance
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Design Principles:**
- **Bulk Processing First**: Optimized for high-volume event batches
- **Client Attribution**: Every event tagged with resolved client_id
- **Write-Optimized**: Minimal indexes, maximum insert performance
- **Export-Focused**: Short-term buffer before client S3 delivery
- **Raw Event Storage**: No processed analytics, complete client flexibility

## ðŸ“Š Core Table Schema

### `events_log` - Bulk Insert Optimized

Primary table handling 200M+ events/month with client attribution:

```sql
CREATE TABLE events_log (
    id SERIAL PRIMARY KEY,
    event_id UUID DEFAULT gen_random_uuid(),
    event_type VARCHAR(50) NOT NULL,           -- 'pageview', 'click', 'batch'
    session_id VARCHAR(100),                   -- User session identifier
    visitor_id VARCHAR(100),                   -- Anonymous visitor ID
    site_id VARCHAR(100),                      -- Domain (e.g., 'shop.acme.com')
    timestamp TIMESTAMPTZ NOT NULL,            -- Event occurrence time
    url TEXT,                                  -- Full page URL
    path VARCHAR(500),                         -- URL path component
    user_agent TEXT,                           -- Browser information
    ip_address INET,                           -- Client IP address
    raw_event_data JSONB NOT NULL,             -- Complete event payload
    created_at TIMESTAMPTZ DEFAULT NOW(),      -- Database insertion time
    processed_at TIMESTAMPTZ                   -- S3 export completion timestamp
);
```

> **Note**: Client attribution is handled at the application level via SQLAlchemy models, which add a `client_id` field not present in the SQL schema.

### Optimized Indexes
```sql
-- Performance indexes for common queries
CREATE INDEX idx_events_log_timestamp ON events_log(timestamp);
CREATE INDEX idx_events_log_site_created ON events_log(site_id, created_at);
CREATE INDEX idx_events_log_session ON events_log(session_id);
CREATE INDEX idx_events_log_event_type ON events_log(event_type);
CREATE INDEX idx_events_log_processed ON events_log(processed_at);

-- JSONB indexes for flexible queries
CREATE INDEX idx_events_log_raw_data_gin ON events_log USING gin(raw_event_data);
```

## âš¡ Bulk Processing Optimization

### Batch Insert Architecture
```sql
-- Traditional approach (slow)
INSERT INTO events_log (event_type, site_id, client_id, event_data) 
VALUES ('click', 'shop.acme.com', 'client_acme_corp', '{"element": "button1"}');
-- Repeat 1000x = 1000 transactions

-- Bulk approach (fast)
INSERT INTO events_log (event_type, site_id, client_id, event_data, batch_id) 
VALUES 
('click', 'shop.acme.com', 'client_acme_corp', '{"element": "button1"}', 'batch_uuid'),
('scroll', 'shop.acme.com', 'client_acme_corp', '{"depth": 25}', 'batch_uuid'),
('form_focus', 'shop.acme.com', 'client_acme_corp', '{"field": "email"}', 'batch_uuid');
-- 1000 events = 1 transaction (1000x faster)
```

### Performance Characteristics
- **Insert Rate**: 5,000+ events/second sustained
- **Batch Size**: 10-50 events optimal
- **Transaction Overhead**: 90% reduction vs. individual inserts
- **Storage Efficiency**: JSONB compression, minimal indexes

## ðŸ”§ Client Attribution Integration

### Domain Resolution Flow
1. Event arrives with `site_id` (e.g., "shop.acme.com")
2. API queries pixel-management: `/api/v1/config/domain/shop.acme.com`
3. Returns `client_id: "client_acme_corp"`
4. Event enriched with client attribution before bulk insert
5. All batch events tagged with same `client_id` and `batch_id`

### Multi-Tenant Data Isolation
```sql
-- Query events for specific client
SELECT event_type, COUNT(*) 
FROM events_log 
WHERE client_id = 'client_acme_corp' 
  AND created_at >= NOW() - INTERVAL '24 hours'
GROUP BY event_type;

-- Client-specific export query
SELECT * FROM events_log 
WHERE client_id = 'client_acme_corp' 
  AND export_status = 'pending'
ORDER BY created_at;
```

## ðŸ“¤ S3 Export Integration

### Export Status Tracking
```sql
-- Mark events as exported
UPDATE events_log 
SET export_status = 'exported', 
    processed_at = NOW() 
WHERE client_id = 'client_acme_corp' 
  AND export_status = 'pending'
  AND created_at < NOW() - INTERVAL '1 hour';

-- Export metrics for billing
SELECT client_id, 
       COUNT(*) as total_events,
       COUNT(CASE WHEN export_status = 'exported' THEN 1 END) as exported_events,
       MAX(processed_at) as last_export
FROM events_log 
WHERE created_at >= DATE_TRUNC('day', NOW())
GROUP BY client_id;
```

### Data Retention Management
```sql
-- Cleanup exported events (configurable retention)
DELETE FROM events_log 
WHERE export_status = 'exported' 
  AND processed_at < NOW() - INTERVAL '7 days';

-- Archive old pending events
UPDATE events_log 
SET export_status = 'archived' 
WHERE export_status = 'pending' 
  AND created_at < NOW() - INTERVAL '30 days';
```

## ðŸ” Performance Monitoring

### Key Performance Queries
```sql
-- Bulk insert verification (all events should have same created_at)
SELECT batch_id, client_id, COUNT(*), MIN(created_at), MAX(created_at)
FROM events_log 
WHERE created_at > NOW() - INTERVAL '1 hour'
  AND batch_id IS NOT NULL
GROUP BY batch_id, client_id
HAVING MIN(created_at) != MAX(created_at);  -- Should return no rows

-- Client attribution success rate
SELECT 
  COUNT(*) FILTER (WHERE client_id IS NOT NULL) * 100.0 / COUNT(*) as attribution_rate,
  COUNT(*) as total_events
FROM events_log 
WHERE created_at > NOW() - INTERVAL '1 hour';

-- Export pipeline health
SELECT export_status, COUNT(*), AVG(EXTRACT(EPOCH FROM (NOW() - created_at))/3600) as avg_age_hours
FROM events_log 
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY export_status;
```

### Database Health Checks
```sql
-- Table size and growth
SELECT 
  pg_size_pretty(pg_total_relation_size('events_log')) as table_size,
  (SELECT COUNT(*) FROM events_log) as total_rows,
  (SELECT COUNT(*) FROM events_log WHERE created_at > NOW() - INTERVAL '24 hours') as last_24h;

-- Index efficiency
SELECT indexname, idx_tup_read, idx_tup_fetch 
FROM pg_stat_user_indexes 
WHERE tablename = 'events_log';
```

## ðŸš€ Scaling Configuration

### PostgreSQL Optimization
```bash
# Small deployment (< 50M events/month)
shared_buffers = 512MB
work_mem = 4MB
maintenance_work_mem = 64MB
effective_cache_size = 1GB

# Medium deployment (50-200M events/month)
shared_buffers = 2GB
work_mem = 8MB
maintenance_work_mem = 256MB
effective_cache_size = 4GB

# Large deployment (200M+ events/month)
shared_buffers = 4GB
work_mem = 16MB
maintenance_work_mem = 512MB
effective_cache_size = 8GB
```

### Connection Pool Scaling
```bash
# Development
max_connections = 100
DATABASE_POOL_SIZE = 10

# Production
max_connections = 200
DATABASE_POOL_SIZE = 30
```

## ðŸ”’ Privacy & Compliance

### GDPR Compliance Features
```sql
-- IP hashing for GDPR clients
UPDATE events_log 
SET ip_address = md5(ip_address::text)::inet 
WHERE privacy_level = 'gdpr' 
  AND ip_address IS NOT NULL;

-- Data subject deletion (by visitor_id)
DELETE FROM events_log 
WHERE visitor_id = 'vis_gdpr_deletion_request'
  AND client_id = 'client_gdpr_compliant';
```

### Audit Trail
```sql
-- Complete event provenance
SELECT client_id, site_id, event_type, created_at, batch_id, export_status
FROM events_log 
WHERE visitor_id = 'vis_audit_request'
ORDER BY created_at;
```

## ðŸ› ï¸ Development Setup

### Local Database
```bash
# Connect to database
docker compose exec postgres psql -U postgres -d postgres

# Verify schema
\dt  -- List tables
\d events_log  -- Describe events_log table

# Test bulk insert
INSERT INTO events_log (event_type, site_id, client_id, batch_id, event_data) 
VALUES 
('test1', 'localhost', 'client_test', gen_random_uuid(), '{"test": true}'),
('test2', 'localhost', 'client_test', gen_random_uuid(), '{"test": true}');
```

### Performance Testing
```bash
# Generate test data
docker compose exec postgres psql -U postgres -d postgres -c "
INSERT INTO events_log (event_type, site_id, client_id, batch_id, event_data)
SELECT 
  'test_event', 
  'localhost', 
  'client_test', 
  gen_random_uuid(), 
  '{\"test\": true}'::jsonb
FROM generate_series(1, 10000);"

# Verify bulk processing performance
docker compose logs fastapi | grep "Bulk inserted"
```

---

**Optimized for 200M+ events/month with complete client attribution and data ownership**